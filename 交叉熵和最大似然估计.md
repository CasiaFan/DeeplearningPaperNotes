机器学习的面试题中经常会被问到交叉熵(cross entropy)和最大似然估计(MLE)或者KL散度有什么关系，查了一些资料发现优化这3个东西其实是等价的。

### 熵和交叉熵

提到交叉熵就需要了解下信息论中熵的定义。信息论认为：

> 确定的事件没有信息，随机事件包含最多的信息。

事件信息的定义为：$I(x)=-log(P(x))$；而熵就是描述信息量：$H(x)=E_{x \sim P}[I(x)]​$

也就是$H(x)=E_{x\sim P}[-log(P(x))]=-\Sigma_xP(x)log(P(x))$。如果log的base是2，熵可以认为是衡量编码对应的信息需要的最少bits数；那么交叉熵就是来衡量用特定的编码方案Q来对分布为P的信息x进行编码时需要的最少的bits数。定义如下：

$H(P, Q)=-\Sigma_xP(x)log(Q(x))$

在深度学习中，**P是gt label的真实分布；Q就是网络学习后输出的分布**。

### 最大似然估计

机器学习中，通过最大似然估计方法使参数为$\hat\Theta$的模型使预测值贴近真实数据的概率最大化，即$\hat\Theta=arg\ max_\theta \Pi_{i=1}^Np(x_i|\Theta)$。实际操作中，连乘很容易出现最大值或最小值溢出，造成计算不稳定，由于log函数的单调性，所以将上式进行取对数取负，最小化**负对数似然**(NLL)的结果与原始式子是一样的，即$\hat \Theta =arg\ min_\Theta - \Sigma_{i=1}^Nlog(p(x_i|\Theta))$.

对模型的预测值进行最大似然估计，

$\hat \Theta =arg\ min_\Theta - \Sigma_{i=1}^Nlog(q(x_i|\Theta))$

​    $=arg\min_\Theta-\Sigma_{x\in X}p(x)log(q(x|\Theta))$

​    $=arg\ min_\Theta H(p, q)$

所以最小化NLL和最小化交叉熵最后达到的效果是一样的。

### KL散度

在深度学习中，KL散度用来评估模型输出的预测值分布与真值分布之间的差异，定义如下：$D_{KL}(P||Q)=E_xlog(P(x)/Q(x))​$

$D_{KL}(P||Q)=\Sigma_{x=1}^NP(x)log(P(x)/Q(x))$

​	   	   $=\Sigma_{x=1}^NP(x)[logP(x)-logQ(x)]$

**注意：**KL散度不是标准的距离，因为不满足互换性，即$D_{KL}(P||Q)\neq D_{KL}(Q||P)$

对于交叉熵：

$H(P, Q) = -\Sigma PlogQ$

 		$= -\Sigma PlogP+\Sigma PlogP-\Sigma PlogQ$

​       	  $= H(P) +\Sigma PlogP/Q$

​		 $=H(P)+D_{KL}(P||Q)$

也就是交叉熵就是真值分布的熵与KL散度的和，而真值的熵是确定的，与模型的参数$\Theta $无关，所以梯度下降求导时$\nabla H(P, Q)=\nabla D_{KL}(P||Q)$，也就是说最小化交叉熵与最小化KL散度是一样的。

### 总结

从优化模型参数角度来说，最小化交叉熵，NLL，KL散度这3种方式对模型参数的更新来说是一样的。从这点来看也解释了为什么在深度学习中交叉熵是非常常用的损失函数的原因了。



